# ğŸ•·ï¸ Web Crawler using Streamlit

This project is a simple web crawler built with Python and Streamlit. It allows users to input a starting URL, crawl linked pages up to a defined limit, extract all `<h1>` titles from the pages, and view or download the data in CSV format.

---

## ğŸš€ Features

- ğŸŒ User-defined start URL
- ğŸ”— Follows hyperlinks recursively
- ğŸ“„ Extracts `<h1>` titles from each page
- ğŸ“‰ Progress tracking with a progress bar
- ğŸ“¥ CSV download of extracted titles
- ğŸ›ï¸ Adjustable max page limit via slider
- ğŸ§ª Built-in browser and test-safe design
- ğŸ–¥ï¸ Deployed using Streamlit with a responsive UI

---

## ğŸ“¸ Demo

![Web Crawler UI Screenshot](screenshot.png) <!-- Replace with actual screenshot file name -->

---

## ğŸ”§ Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/yourusername/web-crawler-streamlit.git
   cd web-crawler-streamlit
